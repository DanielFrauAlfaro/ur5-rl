Run docker:

sudo docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 --rm -it --name docker_rl --net host --cpuset-cpus="0-7" -v ~/:/daniel -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix:rw -v /dev:/dev --user=$(id -u $USER):$(id -g $USER) --privileged docker_rl

--gpus all --pid=host


-------- TFM ---------

Pasos iniciales:
    - Dejar estar el modelo que hice en las prácticas; es orientativo y servirá para copiar algunas funciones pero es
para el tema del TFM necesito otras funcionalidades como la cámara

    - Importar el modelo URDF del UR5e en GeoGrasp a Pybullet 
--> hay que generar un único URDF para TODO el modelo

    - Importar el modelo URDF del objeto (MasterChef Can o Chips Can, es lo mismo)

    - Estudiar las opciones para la cámara:
        - Cámara montada en el robot: 
            - Fácil en el real (ya está montada)
            - Difícil en la simulación (habría que generar un punto de vista a cada TimeStamp)
        - Cámara fija
            - Dificultad para conseguir las transformaciones
                - Esto se puede solucionar: con un QR se puede obtener la transformacion entre dos cámaras si las dos
                ven la imagen desde diferentes puntos de vista -> se obtiene la transformada de cada uno respecto del QR,
                se hacen coincidir los sistemas de coordenadas respecot a una base ortonormal en el QR y ya se pueden hacer
                transformaciones entre los frames de la cámara fija y los de la móvil, que por consecuencia se extrapolan
                a la base del robot.
            - En simulación supongo que se pueden coger directamente las referencias de los objetos



----- 16 / 12 / 23
    Se ha creado el URDF con todo el entorno: mesa, pedestal, robot y pinza con las camaras.

    Modificar el entorno de Gym para que solo acepte entradas en las articulaciones del robot, sin tocar la pinza.
    La pinza se piensa en dejarla a parte
        Habría que cambiar el entorno para aceptar las 6 entradas
        Habría que mirar como afecta eso a los métodos de control de la clase robot.
    Meter la cámara externa y un QR en la mesa, donde está el real más o menos

    Meter la extensión para objetos deformables

    OPCIONAL: mirar de introducir una camara funcional en el extremo --> ayudarse del Chat si eso





----- 19 / 12 / 23 --> Reunión con Santiago

    - Entorno:
        - Plantear una cámara fija como lo estaba haciendo con el QR
        - Ver de hacer la comparación entre una y dos cámaras para el entrenamiento (estéreo)
        - Hacerlo con el BLUE (más adelante)
        
        - Hacerlo en velocidad
        - Hacerlo en cartesiano

    - Entradas: la red puede tener varias entradas:
        - La imagen de la vista sin procesar y la posición de la cámara --> convolucional + mlp?
                                                                        --> YOLO + salida para la velocidad / posicion
        - La imagen segmentada con los dedos, la pinza y el objeto --> misma que arriba
        - Las posiciones de cada elemento en la imagen y la posición de la cámara --> MLP o alguna otra

        - Ver si las imagenes usarlas con RGBd o RGB o d según tenga el caso 1, 2 o 3

    - Arquitectura:
        - Separarlo en módulos: acercarse, coger y levantar / manipular.
        - Para el caso de la de tres dedos y cuatro dedos se puede omitir el agente del medio, porque solo cierran y ya
        - Probar de hacer autogestion, que sepan cuando han terminado de hacer su tarea


    - Sim to Real:
        - Entrenar el algoritmo por refuerzo en simulación
        - Conseguir demostraciones en el real para luego hacer un aprendizaje offline. Se coge la imagen y las trayectorias grabadas y 
        las utilizo de alguna manera para ajustar el modelo.
        

    - Si se hace a lo bruto es probable que tarde mas
    - Si se filtra la entrada, se debería hacer la segmentación
        - En este caso se podría usar una YOLO para la pinza y los dedos en simulación y otra para el real
        - O una YOLO para los objetos (para el dataset de YALE seguro que hay alguna) y otra para la de tres dedos
        - También se podría probar con la imagen segmentada, con los elementos destacados con colorines

    

----- 20 / 12 / 23
    - Se ha incluido la segunda camara y parametrizado para meter cualquier numero de camaras
    - IMPORTANTE: se ha girado la muñeca 180º del original para que pueda ver el QR. Luego se puede girar porque solo hace falta para el inicio, es decir,
    para calcular la transformada entre la camara y la base del robot (que no varia)

    - Se esta intentando hacer la deteccion de colisiones. Con la mesa va bien pero con el objeto ya no.
        - Mirar de incluir el objeto en el URDF pero faltaria aleatoriedad en las muestras


---- 21 / 12 / 23

    Distancia QR: 37.5 desde la parte cercana a mi / alejada del robot           
                  32.5 desde la parte cercana al robot
                  10 x 10 de tamaño

                  21.5 cm desde la base del robot a la mesa

    Se ha probado a hacer las colisiones con objetos separados, es decir, teniendo 3 cuerpos simulados (robot, objeto, mesa) de manera que se puedan
obtener las colisiones entre los cuerpos completos y entre los eslabones de cada uno y entre ellos.

    Se va a obtener la posicion de la camara en el mundo a partir de las imagenes que catpuran las dos camaras
    El codigo esta, faltaria probarlo


---- 26 / 12 / 23

    Se ha implementado el cálculo de las transformaciones.
        Por lo pronto, solamente se tiene el cálculo de la transformacion relativa entre las dos cámaras.
        Se supone que la imagen de la cámara del robot es fija, es decir, la transformacion se realiza respecto al sistema de referencia original de la 
      cámara del robot y la externa esté donde esté --> esto se hace así por varios motivos:
            - En simulación para no generar las posiciones de la cámara mientras se mueve el robot.
            - Porque hay momentos de la manipulacion en los que no se captura el ARUCO con la camara del robot.

        Se espera que esto sea suficiente, pues la transformacion que quedaria seria respecto a la base, que seria siempre la misma al tener la vista del 
      robot fija.

        Se pasan las dos imágenes como realimentación junto con los vectores de realimentación, aunque se espera que solamente sea necesaria la
      externa, aunque se podrían utilizar ambas. 
        Si se usan las dos, se podría calcular por un lado la transformación relativa con la posición original de la camara del robot y tener la imagen
      online de la misma.

    
---- 27 / 12 / 23

    Se ha introducido el fondo al entorno para que el entrenamiento sea lo mas cercano a la realidad

    Se ha empezado a leer literatura sobre las redes neuronales y estrategias para utilizar agentes en el entrenamiento. Algunas ideas:
        - Spatial softmax: en una softmax, se elevan las entradas de la capa para obtener probabilidades. En el spatial, se aplica a cada pixel por
      separado, de manera que lo que se intenta es conseguir zonas de atención.
        Se suele usar con mecansimso de atención y se aplica utilizando la softmax a lo largo de cada una de las dimensiones de la imagen


----- 8 / 01 / 24

    Se ha incluido la transformaciones entre cámaras dentro del espacio de observaciones.
    Se ha creado funciones para obtener la posición y orientacion del objeto. También se ha creado el cambio de coordenadas para obtener los ejes de la 
  orientación correctos para el agarre
        Se debería hacer una metodologia que fuera generica para todos los objetos, sin tener en cuenta sus ejes.
            En el MasterChef can y en los que son latas funciona lo que se ha hecho pero cuando se trata de otro tipo de objetos, como es el caso de las 
          cajas no funciona, porque tienen otros ejes y el cambio ya no funciona
            
            FUERZA BRUTA: hacer una transformacion para cada objeto y tener en cuenta en el código cual se ha importado para hacer el cambio.

            Este enfoque no tendría en cuenta otros agarres posibles que el robot pudiera generar, pero estabilizaría el entrenamiento


# -------------------------------------------------------------------------------------------------------------------------------------------------------
#   IDEA PARA EL ENTRENAMIENTO: una función de recompensa que de +1 si se ejecuta un movimiento que acerque al wrist a la posición indicada             #
#                                                                +1 si se ejecuta un movimiento que acerque el wrist a la orientación indicada          #

    OTRA IDEA: se puede hacer un agente y que primero intente llegar al objeto independientemente de su orientacion con la salida para avisar de que 
            ya ha llegado a la posición (entrenándolo antes con la colisión)
            
                Luego, cuando ya esté entrenado con esta función de recompensa, se vuelva a entrenar con otra de manera que lo que realmente importe es que,
            al cerrar la pinza los tres dedos choquen con el objeto primero. Luego se podría expandir para ver si, al subir, no se cae


    Esto se podría hacer con entrenamiento completamente separados (rollo, la función de aproximación no se utiliza en la de choque o en la de coger el
  objeto de manera eficiente) 
    Otra opción es hacerlo incremental, tener la función de aproximación y choque, después añadirle la de colision de los dedos, luego añadirle la de subir
  el objeto, ...
# -------------------------------------------------------------------------------------------------------------------------------------------------------


