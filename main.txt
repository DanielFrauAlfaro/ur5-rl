Run docker:

sudo docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 --rm -it --name docker_rl --net host --cpuset-cpus="0-7" -v ~/:/daniel -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix:rw -v /dev:/dev --user=$(id -u $USER):$(id -g $USER) --privileged docker_rl

--gpus all --pid=host


-------- TFM ---------

Pasos iniciales:
    - Dejar estar el modelo que hice en las prácticas; es orientativo y servirá para copiar algunas funciones pero es
para el tema del TFM necesito otras funcionalidades como la cámara

    - Importar el modelo URDF del UR5e en GeoGrasp a Pybullet 
--> hay que generar un único URDF para TODO el modelo

    - Importar el modelo URDF del objeto (MasterChef Can o Chips Can, es lo mismo)

    - Estudiar las opciones para la cámara:
        - Cámara montada en el robot: 
            - Fácil en el real (ya está montada)
            - Difícil en la simulación (habría que generar un punto de vista a cada TimeStamp)
        - Cámara fija
            - Dificultad para conseguir las transformaciones
                - Esto se puede solucionar: con un QR se puede obtener la transformacion entre dos cámaras si las dos
                ven la imagen desde diferentes puntos de vista -> se obtiene la transformada de cada uno respecto del QR,
                se hacen coincidir los sistemas de coordenadas respecot a una base ortonormal en el QR y ya se pueden hacer
                transformaciones entre los frames de la cámara fija y los de la móvil, que por consecuencia se extrapolan
                a la base del robot.
            - En simulación supongo que se pueden coger directamente las referencias de los objetos



----- 16 / 12 / 23
    Se ha creado el URDF con todo el entorno: mesa, pedestal, robot y pinza con las camaras.

    Modificar el entorno de Gym para que solo acepte entradas en las articulaciones del robot, sin tocar la pinza.
    La pinza se piensa en dejarla a parte
        Habría que cambiar el entorno para aceptar las 6 entradas
        Habría que mirar como afecta eso a los métodos de control de la clase robot.
    Meter la cámara externa y un QR en la mesa, donde está el real más o menos

    Meter la extensión para objetos deformables

    OPCIONAL: mirar de introducir una camara funcional en el extremo --> ayudarse del Chat si eso





----- 19 / 12 / 23 --> Reunión con Santiago

    - Entorno:
        - Plantear una cámara fija como lo estaba haciendo con el QR
        - Ver de hacer la comparación entre una y dos cámaras para el entrenamiento (estéreo)
        - Hacerlo con el BLUE (más adelante)
        
        - Hacerlo en velocidad
        - Hacerlo en cartesiano

    - Entradas: la red puede tener varias entradas:
        - La imagen de la vista sin procesar y la posición de la cámara --> convolucional + mlp?
                                                                        --> YOLO + salida para la velocidad / posicion
        - La imagen segmentada con los dedos, la pinza y el objeto --> misma que arriba
        - Las posiciones de cada elemento en la imagen y la posición de la cámara --> MLP o alguna otra

        - Ver si las imagenes usarlas con RGBd o RGB o d según tenga el caso 1, 2 o 3

    - Arquitectura:
        - Separarlo en módulos: acercarse, coger y levantar / manipular.
        - Para el caso de la de tres dedos y cuatro dedos se puede omitir el agente del medio, porque solo cierran y ya
        - Probar de hacer autogestion, que sepan cuando han terminado de hacer su tarea


    - Sim to Real:
        - Entrenar el algoritmo por refuerzo en simulación
        - Conseguir demostraciones en el real para luego hacer un aprendizaje offline. Se coge la imagen y las trayectorias grabadas y 
        las utilizo de alguna manera para ajustar el modelo.
        

    - Si se hace a lo bruto es probable que tarde mas
    - Si se filtra la entrada, se debería hacer la segmentación
        - En este caso se podría usar una YOLO para la pinza y los dedos en simulación y otra para el real
        - O una YOLO para los objetos (para el dataset de YALE seguro que hay alguna) y otra para la de tres dedos
        - También se podría probar con la imagen segmentada, con los elementos destacados con colorines

    

----- 20 / 12 / 23
    - Se ha incluido la segunda camara y parametrizado para meter cualquier numero de camaras
    - IMPORTANTE: se ha girado la muñeca 180º del original para que pueda ver el QR. Luego se puede girar porque solo hace falta para el inicio, es decir,
    para calcular la transformada entre la camara y la base del robot (que no varia)

    - Se esta intentando hacer la deteccion de colisiones. Con la mesa va bien pero con el objeto ya no.
        - Mirar de incluir el objeto en el URDF pero faltaria aleatoriedad en las muestras


---- 21 / 12 / 23

    Distancia QR: 37.5 desde la parte cercana a mi / alejada del robot           
                  32.5 desde la parte cercana al robot
                  10 x 10 de tamaño

                  21.5 cm desde la base del robot a la mesa

    Se ha probado a hacer las colisiones con objetos separados, es decir, teniendo 3 cuerpos simulados (robot, objeto, mesa) de manera que se puedan
obtener las colisiones entre los cuerpos completos y entre los eslabones de cada uno y entre ellos.

    Se va a obtener la posicion de la camara en el mundo a partir de las imagenes que catpuran las dos camaras
    El codigo esta, faltaria probarlo


---- 26 / 12 / 23

    Se ha implementado el cálculo de las transformaciones.
        Por lo pronto, solamente se tiene el cálculo de la transformacion relativa entre las dos cámaras.
        Se supone que la imagen de la cámara del robot es fija, es decir, la transformacion se realiza respecto al sistema de referencia original de la 
      cámara del robot y la externa esté donde esté --> esto se hace así por varios motivos:
            - En simulación para no generar las posiciones de la cámara mientras se mueve el robot.
            - Porque hay momentos de la manipulacion en los que no se captura el ARUCO con la camara del robot.

        Se espera que esto sea suficiente, pues la transformacion que quedaria seria respecto a la base, que seria siempre la misma al tener la vista del 
      robot fija.

        Se pasan las dos imágenes como realimentación junto con los vectores de realimentación, aunque se espera que solamente sea necesaria la
      externa, aunque se podrían utilizar ambas. 
        Si se usan las dos, se podría calcular por un lado la transformación relativa con la posición original de la camara del robot y tener la imagen
      online de la misma.