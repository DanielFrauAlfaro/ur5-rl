Run docker:

sudo docker run --shm-size=1g --ulimit memlock=-1 --ulimit stack=67108864 --rm -it --name docker_rl --net host --cpuset-cpus="0-7" -v ~/:/daniel -e DISPLAY=$DISPLAY -v /tmp/.X11-unix:/tmp/.X11-unix:rw -v /dev:/dev --user=$(id -u $USER):$(id -g $USER) --privileged docker_rl

--gpus all --pid=host


-------- TFM ---------

Pasos iniciales:
    - Dejar estar el modelo que hice en las prácticas; es orientativo y servirá para copiar algunas funciones pero es
para el tema del TFM necesito otras funcionalidades como la cámara

    - Importar el modelo URDF del UR5e en GeoGrasp a Pybullet 
--> hay que generar un único URDF para TODO el modelo

    - Importar el modelo URDF del objeto (MasterChef Can o Chips Can, es lo mismo)

    - Estudiar las opciones para la cámara:
        - Cámara montada en el robot: 
            - Fácil en el real (ya está montada)
            - Difícil en la simulación (habría que generar un punto de vista a cada TimeStamp)
        - Cámara fija
            - Dificultad para conseguir las transformaciones
                - Esto se puede solucionar: con un QR se puede obtener la transformacion entre dos cámaras si las dos
                ven la imagen desde diferentes puntos de vista -> se obtiene la transformada de cada uno respecto del QR,
                se hacen coincidir los sistemas de coordenadas respecot a una base ortonormal en el QR y ya se pueden hacer
                transformaciones entre los frames de la cámara fija y los de la móvil, que por consecuencia se extrapolan
                a la base del robot.
            - En simulación supongo que se pueden coger directamente las referencias de los objetos



----- 16 / 12 / 23
    Se ha creado el URDF con todo el entorno: mesa, pedestal, robot y pinza con las camaras.

    Modificar el entorno de Gym para que solo acepte entradas en las articulaciones del robot, sin tocar la pinza.
    La pinza se piensa en dejarla a parte
        Habría que cambiar el entorno para aceptar las 6 entradas
        Habría que mirar como afecta eso a los métodos de control de la clase robot.
    Meter la cámara externa y un QR en la mesa, donde está el real más o menos

    Meter la extensión para objetos deformables

    OPCIONAL: mirar de introducir una camara funcional en el extremo --> ayudarse del Chat si eso





----- 19 / 12 / 23 --> Reunión con Santiago

    - Entorno:
        - Plantear una cámara fija como lo estaba haciendo con el QR
        - Ver de hacer la comparación entre una y dos cámaras para el entrenamiento (estéreo)
        - Hacerlo con el BLUE (más adelante)
        
        - Hacerlo en velocidad
        - Hacerlo en cartesiano

    - Entradas: la red puede tener varias entradas:
        - La imagen de la vista sin procesar y la posición de la cámara --> convolucional + mlp?
                                                                        --> YOLO + salida para la velocidad / posicion
        - La imagen segmentada con los dedos, la pinza y el objeto --> misma que arriba
        - Las posiciones de cada elemento en la imagen y la posición de la cámara --> MLP o alguna otra

        - Ver si las imagenes usarlas con RGBd o RGB o d según tenga el caso 1, 2 o 3

    - Arquitectura:
        - Separarlo en módulos: acercarse, coger y levantar / manipular.
        - Para el caso de la de tres dedos y cuatro dedos se puede omitir el agente del medio, porque solo cierran y ya
        - Probar de hacer autogestion, que sepan cuando han terminado de hacer su tarea


    - Sim to Real:
        - Entrenar el algoritmo por refuerzo en simulación
        - Conseguir demostraciones en el real para luego hacer un aprendizaje offline. Se coge la imagen y las trayectorias grabadas y 
        las utilizo de alguna manera para ajustar el modelo.
        

    - Si se hace a lo bruto es probable que tarde mas
    - Si se filtra la entrada, se debería hacer la segmentación
        - En este caso se podría usar una YOLO para la pinza y los dedos en simulación y otra para el real
        - O una YOLO para los objetos (para el dataset de YALE seguro que hay alguna) y otra para la de tres dedos
        - También se podría probar con la imagen segmentada, con los elementos destacados con colorines

    

----- 20 / 12 / 23
    - Se ha incluido la segunda camara y parametrizado para meter cualquier numero de camaras
    - IMPORTANTE: se ha girado la muñeca 180º del original para que pueda ver el QR. Luego se puede girar porque solo hace falta para el inicio, es decir,
    para calcular la transformada entre la camara y la base del robot (que no varia)

    - Se esta intentando hacer la deteccion de colisiones. Con la mesa va bien pero con el objeto ya no.
        - Mirar de incluir el objeto en el URDF pero faltaria aleatoriedad en las muestras